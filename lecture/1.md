# Конспект лекции №1

**Обработка и интерпретация сигналов**

**Цель курса**\
Все данные, для которых существует возможность потери или искажения,
всегда кодируются с целью уменьшения влияния этих искажений.

Изучение теории передачи данных через каналы с помехами и способов
канального кодирования, позволяющих добавлять избыточность для борьбы с
ошибками.

## 1. Упрощённая модель цифровой системы связи

Источник данных может быть: - человек; - флеш-накопитель; - компакт-диск
и т.п.

    Источник данных  
        ↓ (информационные символы)  
    Кодер канала  
        ↓ (кодовые символы с избыточностью)  
    Канал с шумом  
        ↓ (кодовые символы + ошибки)  
    Декодер  
        ↓ (оценка информационных символов)  
    Получатель

-   **Источник данных** --- генерирует информационные символы (обычно
    биты 0/1).\
-   **Кодер канала** --- добавляет избыточность → кодовые символы.\
-   **Канал** --- вносит ошибки.\
-   **Декодер** --- пытается восстановить исходные информационные
    символы.

## 2. Основные допущения модели

1.  Работаем с **дискретными последовательностями**.\

2.  Информационная последовательность состоит из элементов поля

    **$GF(2) = \{0, 1\}$**

Источник генерирует последовательность нулей и единиц.\
После кодирования на выходе также получаются битовые символы, но, как
правило,\
их становится больше за счёт избыточности.

------------------------------------------------------------------------

## 3. Двоичный симметричный канал (ДСК)

В качестве математической модели канала используется\
**двоичный симметричный канал**.

На вход канала последовательно подаются символы $0$ и $1$.

### Переходная вероятность

Поданный на канал сигнал может быть интерпретирован неверно\
с вероятностью $p$, называемой **переходной вероятностью**.

**Двоичный симметричный канал (ДСК / BSC)**

    Вход → [Канал] → Выход
      0  ──(1−p)──→  0
      0  ────p────→  1
      1  ────p────→  0
      1  ──(1−p)──→  1

Если кодируется один символ без избыточности,\
вероятность ошибки равна $p$.

------------------------------------------------------------------------

## 4. Канальное кодирование

Процедура перевода информационных символов\
в кодовые символы называется **кодированием**.

### 4.1. Повторный (тройной) код

$$
0 \rightarrow 000, \qquad
1 \rightarrow 111
$$

-   Скорость кода: $R = \frac{1}{3}$\
-   Исправляет одну ошибку.\
-   Декодирование производится по правилу большинства.

------------------------------------------------------------------------

### 4.2. Линейный $(5,2)$-код

$$
\begin{aligned}
00 &\rightarrow 00000 \\
01 &\rightarrow 10110 \\
10 &\rightarrow 01011 \\
11 &\rightarrow 11101
\end{aligned}
$$

Свойства кода: - код линейный; - исправляет одну ошибку; - скорость
кода: $$
R = \frac{k}{n} = \frac{2}{5}
$$

**Домашнее задание:**\
вычислить вероятность ошибки декодирования при $p = 10^{-3}$.

------------------------------------------------------------------------

## 5. Теорема Шеннона (1948)

Теория информации делится на: - кодирование источников (удаление
избыточности); - канальное кодирование (добавление избыточности).

Пропускная способность двоичного симметричного канала:

$$
C = 1 - h_2(p)
$$

где

$$
h_2(p) = -p \log_2 p - (1-p) \log_2 (1-p)
$$

### Утверждение

-   если $R < C$, вероятность ошибки может быть сколь угодно малой;
-   если $R > C$, надёжная передача невозможна.

------------------------------------------------------------------------

## 6. Вес и расстояние Хэмминга

**Вес Хэмминга**: $$
\omega(x)
$$ --- число единиц в двоичном векторе $x$.

**Расстояние Хэмминга**: $$
d(x,y) = \omega(x+y)
$$

Также: $$
d(x,0) = \omega(x)
$$

**Пример:**

$$
x = 001101, \quad \omega(x)=3
$$ $$
y = 101001, \quad \omega(y)=3
$$ $$
d(x,y)=2
$$

------------------------------------------------------------------------

## 7. Минимальное расстояние кода

$$
d_{\min} = \min_{x \ne y} d(x,y)
$$

------------------------------------------------------------------------

## 8. Исправляющая способность

$$
t \le \left\lfloor \frac{d_{\min}-1}{2} \right\rfloor
$$

------------------------------------------------------------------------

## 9. Линейные коды

Код $C$ называется линейным, если:

$$
\forall x,y \in C : x+y \in C
$$

Для линейных кодов:

$$
d_{\min} = \min_{z \in C,\, z \ne 0} \omega(z)
$$

------------------------------------------------------------------------

## 10. $q$-ичные линейные коды

Линейный $(n,k)$-код над полем $GF(q)$ ---\
это $k$-мерное подпространство пространства $GF(q)^n$.

**Пример:**\
$q=3,\ k=2,\ n=5$

Число информационных слов: $$
q^k = 9
$$

------------------------------------------------------------------------

## 11. Порождающая матрица

Порождающая матрица $G$ имеет размер $k \times n$.

Информационное слово: $$
m = (m_1, \dots, m_k)
$$

Кодовое слово: $$
c = m \cdot G
$$

------------------------------------------------------------------------

## 12. Проверочная матрица

Пусть существует вектор $$
h = (h_1, \dots, h_n)
$$

такой, что: $$
(c,h) = \sum_{i=1}^n c_i h_i = 0
$$

Строится проверочная матрица $H$ размера $(n-k) \times n$,\
для которой выполняется:

$$
G \cdot H^T = 0
$$

------------------------------------------------------------------------

## 13. Сводка формул

-   $\omega(x)$ --- вес Хэмминга\
-   $d(x,y)=\omega(x+y)$ --- расстояние\
-   $d_{\min}$ --- минимальное расстояние\
-   $t \le \left\lfloor \frac{d_{\min}-1}{2} \right\rfloor$\
-   $c = m \cdot G$\
-   $G \cdot H^T = 0$\
-   $R = \frac{k}{n}$
